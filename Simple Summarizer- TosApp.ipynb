{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/anaconda/lib/python2.7/site-packages/gensim-0.13.3-py2.7-macosx-10.6-x86_64.egg/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import nltk, re, numpy as np\n",
    "from nltk import word_tokenize\n",
    "from gensim.summarization import summarize, keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### webscrape the terms of service page from the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "raw_html = requests.get('https://www.google.com/policies/terms/').text.encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open('twitter_tos.htm', 'r') as f:\n",
    "#    raw_html = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Use Beautiful Soup package to extract text from the html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input = BeautifulSoup(raw_html,\"lxml\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Use Regular expression to remove weblinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "input = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input=input.encode(\"ascii\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple Summarizer\n",
    "# Copyright (C) 2010-2012 Tristan Havelick\n",
    "# Author: Tristan Havelick <tristan@havelick.com>\n",
    "# URL: <https://github.com/thavelick/summarize/>\n",
    "# For license information, see LICENSE.TXT\n",
    "\n",
    "\n",
    "class SimpleSummarizer:\n",
    "\n",
    "\tdef reorder_sentences( self, output_sentences, input ):\n",
    "\t\toutput_sentences.sort( lambda s1, s2:\n",
    "\t\t\tinput.find(s1) - input.find(s2) )\n",
    "\t\treturn output_sentences\n",
    "\t\n",
    "\tdef get_summarized(self, input, num_sentences ):\n",
    "\t\t# TODO: allow the caller to specify the tokenizer they want\n",
    "\t\t# TODO: allow the user to specify the sentence tokenizer they want\n",
    "\t\t\n",
    "\t\ttokenizer = RegexpTokenizer('\\w+')\n",
    "\t\t\n",
    "\t\t# get the frequency of each word in the input\n",
    "\t\tbase_words = [word.lower() for word in tokenizer.tokenize(input)]\n",
    "\t\twords = [word for word in base_words if word not in stopwords.words()]\n",
    "\t\tword_frequencies = FreqDist(words)\n",
    "\t\t\n",
    "\t\t# now create a set of the most frequent words\n",
    "\t\tmost_frequent_words = [pair[0] for pair in\n",
    "\t\t\tword_frequencies.items()[:100]]\n",
    "\t\t\n",
    "\t\t# break the input up into sentences.  working_sentences is used\n",
    "\t\t# for the analysis, but actual_sentences is used in the results\n",
    "\t\t# so capitalization will be correct.\n",
    "\t\t\n",
    "\t\tsent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\t\tactual_sentences = sent_detector.tokenize(input)\n",
    "\t\tworking_sentences = [sentence.lower() for sentence in actual_sentences]\n",
    "\n",
    "\t\t# iterate over the most frequent words, and add the first sentence\n",
    "\t\t# that inclues each word to the result.\n",
    "\t\toutput_sentences = []\n",
    "\n",
    "\t\tfor word in most_frequent_words:\n",
    "\t\t\tfor i in range(0, len(working_sentences)):\n",
    "\t\t\t\tif (word in working_sentences[i]\n",
    "\t\t\t\t  and actual_sentences[i] not in output_sentences):\n",
    "\t\t\t\t\toutput_sentences.append(actual_sentences[i])\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tif len(output_sentences) >= num_sentences: break\n",
    "\t\t\tif len(output_sentences) >= num_sentences: break\n",
    "\t\t\t\n",
    "\t\t# sort the output sentences back to their original order\n",
    "\t\treturn self.reorder_sentences(output_sentences, input)\n",
    "\t\n",
    "\tdef summarize(self, input, num_sentences):\n",
    "\t\treturn self.get_summarized(input, num_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create object of SimpleSummarizer Class and pass the text. Number of Output sentences as 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss=SimpleSummarizer()\n",
    "summary=ss.summarize(input,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using our Services\n",
      "You must follow any policies made available to you within the Services.\n",
      "\n",
      "In connection with your use of the Services, we may send you service announcements, administrative messages, and other information.\n",
      "\n",
      "Do not use such Services in a way that distracts you and prevents you from obeying traffic or safety laws.\n",
      "\n",
      "The rights you grant in this license are for the limited purpose of operating, promoting, and improving our Services, and to develop new ones.\n",
      "\n",
      "Our automated systems analyze your content (including emails) to provide you personally relevant product features, such as customized search results, tailored advertising, and spam and malware detection.\n",
      "\n",
      "If you have a Google Account, we may display your Profile name, Profile photo, and actions you take on Google or on third-party applications connected to your Google Account (such as +1s, reviews you write and comments you post) in our Services, including displaying in ads and other commercial contexts.\n",
      "\n",
      "You may not copy, modify, distribute, sell, or lease any part of our Services or included software, nor may you reverse engineer or attempt to extract the source code of that software, unless laws prohibit those restrictions or you have our written permission.\n",
      "\n",
      "You can stop using our Services at any time, although well be sorry to see you go.\n",
      "\n",
      "It will hold harmless and indemnify Google and its affiliates, officers, agents, and employees from any claim, suit or action arising from or related to the use of the Services or violation of these terms, including any liability or expense arising from claims, losses, damages, suits, judgments, litigation costs and attorneys fees.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newsummary=[]\n",
    "for sent in summary[1:]:\n",
    "    print sent\n",
    "    print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
