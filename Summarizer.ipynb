{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains our summarization techniques that we tried ranging from Keyphrase Extraction (using frequency, collocation, chunking and wordnet) to Key sentences extraction ( sentences with most frequent words, Gensim summarizer, Classification and Gensim summarizer). The final approach that we used was Key sentences extraction using Classification and Gensim Summarization that uses TextRank algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "import nltk, re, numpy as np\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "from gensim.summarization import summarize, keywords\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the files\n",
    "The following sections read in the text from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loadText\n",
    "def loadText(text):\n",
    "    f = open(text)\n",
    "    raw = f.read().decode('utf8');\n",
    "    return raw\n",
    "\n",
    "tos_text = loadText(\"static/data/Google.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = tos_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyphrase Extraction Approach\n",
    "\n",
    "In the following approaches - frequency, collocation and chunking - for each, the steps followed are: 1. Identifying candidates for keyphrases and 2. Keyphrase selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1 - Frequency\n",
    "\n",
    "The following segment attempts using Approach 1 - Frequent Terms\n",
    "-- Frequent Unigrams (with or without stemming, stopwords, and other normalization)\n",
    "-- Frequent Bigram frequencies (with or without stemming, stopwords, and other normalization)\n",
    "-- Other variations on frequent n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Candidates identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Most frequent tokens: \n",
      "[(',', 242), ('.', 188), ('to', 152), ('and', 124), ('you', 120), ('your', 114), ('Google', 113), ('information', 102), ('or', 71), ('our', 65), ('the', 62), ('with', 60), ('services', 59), ('we', 59), ('of', 57), ('a', 45), ('more', 45), ('may', 42), ('that', 40), ('in', 39)]\n"
     ]
    }
   ],
   "source": [
    "#Using FreqDist\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "text_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "fdist = nltk.FreqDist(text_tokens)\n",
    "print(\"20 Most frequent tokens: \")\n",
    "print(fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing\n",
    "Normalizing - convert to lower case, remove punctuation and stop words, stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punct = set(string.punctuation)\n",
    "#To remove specific punctuation and possessive found in this book\n",
    "all_punct=string.punctuation + \"--\" + \".\\\"\" + \",\\\"\" + \"?\\\"\" + \"\\'s\" + \"!\\\"\"+\"\\\"\"\n",
    "text_nopunct = [x.lower() for x in text_tokens if x not in all_punct]      #Convert to lower & punctuation\n",
    "#print(\"Total number of words: \", len(text_nopunct),\"\\n\")\n",
    "\n",
    "norm_fdist = nltk.FreqDist(text_nopunct)\n",
    "unique_incl_stop = len(norm_fdist.keys())\n",
    "#print(\"Total number of unique words - without removing stop words: \", unique_incl_stop)\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "norm = [i for i in text_nopunct if i not in stop]    #Remove stopwords\n",
    "norm_fdist = nltk.FreqDist(norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Keyphrase selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized Text:\n",
      "\n",
      "20 Most frequent words: \n",
      "[('google', 113), ('information', 110), ('services', 59), ('may', 42), ('example', 38), ('account', 35), ('use', 35), ('learn', 34), ('privacy', 30), ('personal', 24), ('like', 19), ('policy', 18), ('collect', 18), ('device', 16), ('ads', 16), ('cookies', 16), ('access', 16), ('including', 16), ('share', 16), ('people', 13)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNormalized Text:\")\n",
    "#print(\"Total number of unique words post normalizing: \",len(norm_fdist.keys()))\n",
    "print(\"\\n20 Most frequent words: \")\n",
    "print(norm_fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Using stemming on unigrams, post normalizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using stemmed words: \n",
      "\n",
      "20 Most frequent words: \n",
      "[('googl', 113), ('inform', 111), ('servic', 67), ('use', 60), ('may', 42), ('exampl', 39), ('account', 35), ('learn', 34), ('privaci', 30), ('person', 27), ('share', 25), ('includ', 24), ('polici', 20), ('like', 20), ('cooki', 20), ('collect', 20), ('devic', 19), ('ad', 19), ('user', 18), ('access', 18)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Results post stemming - Using Snowball stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "text_stem = [stemmer.stem(i) for i in norm]\n",
    "stem_fdist = nltk.FreqDist(text_stem)\n",
    "print(\"\\nUsing stemmed words: \\n\")\n",
    "print(\"20 Most frequent words: \")\n",
    "print(stem_fdist.most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output is not very useful. It basically used all the words and even post normalizing did not provide a very meaningful output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the above, but using bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 Most frequent bigrams: \n",
      "\n",
      "[(('personal', 'information'), 23), (('google', 'account'), 23), (('privacy', 'policy'), 17), (('google', 'analytics'), 10), (('use', 'services'), 8), (('advertising', 'services'), 7), (('associated', 'google'), 7), (('information', 'collect'), 7), (('share', 'information'), 7), (('example', 'google'), 6), (('search', 'results'), 6), (('use', 'information'), 6), (('google', 'services'), 6), (('services', 'may'), 5), (('information', 'google'), 5), (('many', 'services'), 5), (('services', 'google'), 4), (('cookies', 'similar'), 4), (('use', 'google'), 4), (('information', 'publicly'), 4)]\n",
      "\n",
      "\n",
      "20 Most frequent trigrams: \n",
      "\n",
      "[(('associated', 'google', 'account'), 7), (('cookies', 'similar', 'technologies'), 4), (('personal', 'information', 'companies'), 3), (('organizations', 'individuals', 'outside'), 3), (('companies', 'organizations', 'individuals'), 3), (('share', 'personal', 'information'), 3), (('relevant', 'search', 'results'), 3), (('process', 'enforceable', 'governmental'), 3), (('google', 'account', 'learn'), 3), (('enforceable', 'governmental', 'request'), 3), (('personal', 'information', 'google'), 3), (('information', 'companies', 'organizations'), 3), (('information', 'associated', 'google'), 3), (('collect', 'store', 'information'), 3), (('individuals', 'outside', 'google'), 3), (('information', 'one', 'service'), 2), (('information', 'visits', 'multiple'), 2), (('security', 'related', 'materials'), 2), (('privacy', 'policy', 'changes'), 2), (('law', 'regulation', 'legal'), 2)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "\n",
    "bi = bigrams(norm)\n",
    "bi_list = [bigram for bigram in bi]\n",
    "tri = trigrams(norm)\n",
    "tri_list = [trigram for trigram in tri]\n",
    "\n",
    "bi_fdist = nltk.FreqDist(bi_list)\n",
    "print(\"\\n20 Most frequent bigrams: \\n\")\n",
    "print(bi_fdist.most_common(20))\n",
    "print(\"\\n\\n20 Most frequent trigrams: \\n\")\n",
    "tri_fdist = nltk.FreqDist(tri_list)\n",
    "print(tri_fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2 - Collocation (Words)\n",
    "\n",
    "Collocation can be done either with words or with parts of speech. Here, I have attempted with words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Candidate Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Candidate Selection - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 bigrams using PMI:\n",
      "[('cell', 'towers'), ('enforceable', 'governmental'), ('pixel', 'tags'), ('credit', 'card'), ('domain', 'administrator'), ('individuals', 'outside'), ('organizations', 'individuals'), ('governmental', 'request'), ('companies', 'organizations'), ('ip', 'addresses')]\n",
      "\n",
      "Top 10 bigrams using Pearson's Chi-Squared Test:\n",
      "[('cell', 'towers'), ('enforceable', 'governmental'), ('pixel', 'tags'), ('domain', 'administrator'), ('credit', 'card'), ('companies', 'organizations'), ('individuals', 'outside'), ('organizations', 'individuals'), ('privacy', 'policy'), ('governmental', 'request')]\n"
     ]
    }
   ],
   "source": [
    "#Post normalizing\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(norm)\n",
    "\n",
    "# only bigrams that appear atleast 3 times\n",
    "finder.apply_freq_filter(3)\n",
    "\n",
    "# returns the 10 bigrams with the highest PMI\n",
    "print(\"\\nTop 10 bigrams using PMI:\")\n",
    "print(finder.nbest(bigram_measures.pmi, 10))\n",
    "#finder.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "# Finds top 10 bigrams using the Pearson's Chi-squared test\n",
    "print(\"\\nTop 10 bigrams using Pearson's Chi-Squared Test:\")\n",
    "print(finder.nbest(bigram_measures.chi_sq, 10))\n",
    "#finder.score_ngrams(bigram_measures.chi_sq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 bigrams using Maximum Likelihood Ratio:\n",
      "[('privacy', 'policy'), ('personal', 'information'), ('google', 'account'), ('search', 'results'), ('domain', 'administrator'), ('companies', 'organizations'), ('google', 'analytics'), ('cell', 'towers'), ('enforceable', 'governmental'), ('pixel', 'tags')]\n"
     ]
    }
   ],
   "source": [
    "# Finds top 10 bigrams using the likelihood ratio\n",
    "\n",
    "print(\"\\nTop 10 bigrams using Maximum Likelihood Ratio:\")\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 10))\n",
    "#finder.score_ngrams(bigram_measures.likelihood_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I also used collocations on text that was not normalized but that did not yield in meaningful results since the stop words were included and ended up being among the most frequent.\n",
    "Eg: ('of', 'the'), ('in', 'the'), ('the', 'jungle'), ('he', 'was'), etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Candidate Selection - Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 trigrams using PMI:\n",
      "[('enforceable', 'governmental', 'request'), ('organizations', 'individuals', 'outside'), ('process', 'enforceable', 'governmental'), ('companies', 'organizations', 'individuals'), ('relevant', 'search', 'results'), ('cookies', 'similar', 'technologies'), ('individuals', 'outside', 'google'), ('information', 'companies', 'organizations'), ('collect', 'store', 'information'), ('personal', 'information', 'companies')]\n",
      "\n",
      "Top 10 trigrams using Pearson's Chi-Squared Test:\n",
      "[('enforceable', 'governmental', 'request'), ('organizations', 'individuals', 'outside'), ('process', 'enforceable', 'governmental'), ('companies', 'organizations', 'individuals'), ('cookies', 'similar', 'technologies'), ('relevant', 'search', 'results'), ('individuals', 'outside', 'google'), ('information', 'companies', 'organizations'), ('associated', 'google', 'account'), ('collect', 'store', 'information')]\n"
     ]
    }
   ],
   "source": [
    "# Post normalizing\n",
    "\n",
    "finder = TrigramCollocationFinder.from_words(norm)\n",
    "\n",
    "# only trigrams that appear atleast 3 times\n",
    "finder.apply_freq_filter(3)\n",
    "\n",
    "# return the 10 trigrams with the highest PMI\n",
    "print(\"\\nTop 10 trigrams using PMI:\")\n",
    "print(finder.nbest(trigram_measures.pmi, 10))\n",
    "#finder.score_ngrams(trigram_measures.pmi)\n",
    "\n",
    "# Finds top 10 trigrams using the Pearson's Chi-squared test\n",
    "print(\"\\nTop 10 trigrams using Pearson's Chi-Squared Test:\")\n",
    "print(finder.nbest(trigram_measures.chi_sq, 10))\n",
    "#finder.score_ngrams(trigram_measures.chi_sq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 trigrams using Maximum Likelihood Ratio:\n",
      "[('personal', 'information', 'companies'), ('associated', 'google', 'account'), ('share', 'personal', 'information'), ('personal', 'information', 'google'), ('google', 'account', 'learn'), ('relevant', 'search', 'results'), ('companies', 'organizations', 'individuals'), ('enforceable', 'governmental', 'request'), ('cookies', 'similar', 'technologies'), ('process', 'enforceable', 'governmental')]\n"
     ]
    }
   ],
   "source": [
    "# Finds top 10 trigrams using the likelihood ratio\n",
    "print(\"\\nTop 10 trigrams using Maximum Likelihood Ratio:\")\n",
    "print(finder.nbest(trigram_measures.likelihood_ratio, 10))\n",
    "#finder.score_ngrams(trigram_measures.likelihood_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproach 3 - Chunking\n",
    "\n",
    "### 1. Candidate Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences  \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "def create_chunker(grammar):\n",
    "    return nltk.RegexpParser(grammar)\n",
    "\n",
    "def run_chunker(ch, sentences):\n",
    "    return [ch.parse(sent) for sent in sentences]\n",
    "\n",
    "# Defining the grammar for the chunker\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+}  # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "\"\"\"\n",
    "tagged_sentences = nltk.pos_tag_sents(tokenize_text(text))\n",
    "\n",
    "clause_chunker = create_chunker(grammar)\n",
    "\n",
    "# Collecting the clauses\n",
    "clauses = []\n",
    "for sent in tagged_sentences:\n",
    "    tree = clause_chunker.parse(sent)\n",
    "    for st in tree.subtrees():\n",
    "        if st.label() == 'CLAUSE': clauses.append(st)\n",
    "\n",
    "#print(clauses)\n",
    "\n",
    "#Collecting the proper nouns\n",
    "proper_nouns = []\n",
    "for sent in tagged_sentences:\n",
    "    tree = clause_chunker.parse(sent)\n",
    "    for st in tree.subtrees():\n",
    "        if st.label() == 'NP': proper_nouns.append(st.leaves()[0][0])\n",
    "#print(proper_nouns)\n",
    "\n",
    "candidates = []\n",
    "for clause in clauses:\n",
    "    for s in clause.subtrees():\n",
    "        if(s.label() == 'NP'):\n",
    "            for l in s.leaves():\n",
    "                for word in l:\n",
    "                    if(word in proper_nouns):\n",
    "                        candidates.append(clause)\n",
    "                        next\n",
    "\n",
    "candidate_sentences = []\n",
    "for candidate in candidates:\n",
    "    candidate_sentences.append(' '.join([l[0] for l in candidate.leaves()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Candidate Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legal team reviews each\n",
      "the ads delivered by Google\n",
      "we’re using information\n",
      "partners – like publishers\n",
      "security related materials\n",
      "Data generated through Google Analytics\n",
      "Google Analytics product helps businesses\n",
      "The hyperlinked examples\n",
      "don’t follow the correct process\n",
      "SMS routing information\n",
      "choice People have different privacy concerns\n",
      "services appear in the language\n",
      "services using SSL\n",
      "partners use various technologies\n",
      "Specific product practices The following notices explain specific privacy practices with respect\n",
      "device using mechanisms such as browser web storage\n",
      "a partner uses Google Analytics in conjunction\n",
      "Google uses cookies\n",
      "other Google­hosted content\n",
      "This includes information\n",
      "Unique application numbers Certain services include a unique application number\n",
      "Google processes personal information\n",
      "view archived versions\n",
      "view content provided by Google\n",
      "site owners analyze the traffic\n",
      "services offered on other sites\n",
      "Profile photo appear in shared endorsements\n",
      "services offered by other companies\n",
      "mobile network information including phone number\n",
      "we’ll be able\n",
      "device’s location based on the\n",
      "own services is Google Analytics\n",
      "services Google provides on Android devices\n",
      "Device identifiers let Google\n"
     ]
    }
   ],
   "source": [
    "lchar = 0\n",
    "finlist = []\n",
    "\n",
    "for sentence in set(candidate_sentences):\n",
    "    if lchar<2000:\n",
    "        ls = len(sentence)\n",
    "        if (lchar + ls)<2000:\n",
    "            finlist.append(sentence)\n",
    "            lchar +=ls\n",
    "        \n",
    "print(\"\\n\".join(ph for ph in finlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproach 4 - Wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text_semantic(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences    \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('google_privacy.txt','r') as fs:\n",
    "        summaries_str = fs.read()\n",
    "        \n",
    "sents = tokenize_text_semantic(summaries_str)\n",
    "tagged_POS_sents = [nltk.pos_tag(sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def freq_normed_unigrams(tagged_sents, num):\n",
    "    wnl = WordNetLemmatizer() # to get word stems      \n",
    "    normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_sents\n",
    "                           for word in sent \n",
    "                           if word[0].lower() not in stopwords.words('English')\n",
    "                           and word[0] not in string.punctuation # remove punctuation\n",
    "                           and not re.search(r'''^[\\.,;\"'?!():\\-_`–—]+$''', word[0])\n",
    "                           and word[1].startswith('N')]  # include only nouns\n",
    "    top_normed_unigrams = [word for (word, count) in nltk.FreqDist(normed_tagged_words).most_common(num)]\n",
    "    return top_normed_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "\n",
    "def categories_from_hypernyms(tagged_sents, num=10):\n",
    "    termlist = freq_normed_unigrams(tagged_sents, num) # get top unigrams\n",
    "    hypterms = []\n",
    "    key_hyponyms = []\n",
    "    hyp_list = []\n",
    "    hypterms_dict = defaultdict(list)\n",
    "    for term in termlist:                  # for each term\n",
    "        s = wn.synsets(term.lower(), 'n')  # get its nominal synsets\n",
    "        for syn in s:                      # for each lemma synset\n",
    "            for hyp in syn.hypernyms():    # It has a list of hypernyms\n",
    "                hypterms = hypterms + [hyp.name]      # Extract the hypernym name and add to list\n",
    "                hypterms_dict[hyp.name].append(term)  # Extract examples and add them to dict\n",
    "    hypfd = nltk.FreqDist(hypterms)             # After going through all the nouns, print out the hypernyms \n",
    "    for (name, count) in hypfd.most_common(10):  # that have accumulated the most counts (have seen the most descendents)\n",
    "        key_hyponyms.append(hypterms_dict[name])\n",
    "        # here I eliminated her category listing, opting instead to flattern out the listing and show synonyms\n",
    "    top_hyponyms = [item for sublist in key_hyponyms for item in sublist]\n",
    "    fd_hyp = nltk.FreqDist(top_hyponyms).most_common()\n",
    "    for (each, count) in fd_hyp:\n",
    "        hyp_list.append(each)\n",
    "    #print(hyp_list)\n",
    "    return hyp_list\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key_topics = categories_from_hypernyms(tagged_POS_sents, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KEY TOPICS:', 'account', 'share', 'access', 'user', 'information', 'device']\n"
     ]
    }
   ],
   "source": [
    "output_key_topics = ['KEY TOPICS:'] + [each for each in categories_from_hypernyms(tagged_POS_sents, 15)]\n",
    "print(output_key_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflection on using Keyphrase for ToS summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used four approaches to identify keyphrases - Using frequencies, collocations, chunking and wordnet - on the Terms of Service, of Google - as an example. \n",
    "\n",
    "#### Frequencies\n",
    "Using frequencies was a very simple and straightforward approach. However, it did not give very much information. Even post normalizing the text by removing punctuation, converting to lower case and stemming, there's not much improvement in the quality of words recognized.\n",
    "For Eg: \n",
    "20 Most frequent words: \n",
    "[('google', 113), ('information', 110), ('services', 59), ('may', 42), ('example', 38), ('account', 35), ('use', 35), ('learn', 34), ('privacy', 30), ('personal', 24), ('like', 19), ('policy', 18), ('collect', 18), ('device', 16), ('ads', 16), ('cookies', 16), ('access', 16), ('including', 16), ('share', 16), ('people', 13)]\n",
    "\n",
    "Applying frequencies to bigrams and trigrams resulted in some of the common phrases being bundled together. However, the quality of this improvement was not very much. While using the text in its actual form would make for more meaningful bigrams and trigrams (with prepositions and articles), these occur very frequently and hence shadow the others. Hence, we used the normalized text for bigrams and trigrams.\n",
    "For Eg:\n",
    "[(('personal', 'information'), 23), (('google', 'account'), 23), (('privacy', 'policy'), 17), (('google', 'analytics'), 10), (('use', 'services'), 8), (('advertising', 'services'), 7), (('associated', 'google'), 7), (('information', 'collect'), 7), (('share', 'information'), 7), (('example', 'google'), 6), (('search', 'results'), 6), (('use', 'information'), 6), (('google', 'services'), 6), (('services', 'may'), 5), (('information', 'google'), 5), (('many', 'services'), 5), (('services', 'google'), 4), (('cookies', 'similar'), 4), (('use', 'google'), 4), (('information', 'publicly'), 4)]\n",
    "\n",
    "As seen above, while these are key terms (personal information, etc.), they do not tell us anything about the context of use and hence are not very useful.\n",
    "\n",
    "\n",
    "#### Collocations\n",
    "We implemented collocations using bigrams and trigrams of the words, using the statistical measures of Pearson Chi Square and PMI. However, both these did not provide very meaningful information across both bigrams and trigrams. We then implemented maximum likelihood ratio and this worked relatively well compared to the others for bigrams. The result for bigrams was better than that for trigrams.\n",
    "Eg:Bigrams: Top 10 bigrams using Maximum Likelihood Ratio:\n",
    "[('privacy', 'policy'), ('personal', 'information'), ('google', 'account'), ('search', 'results'), ('domain', 'administrator'), ('companies', 'organizations'), ('google', 'analytics'), ('cell', 'towers'), ('enforceable', 'governmental'), ('pixel', 'tags')]\n",
    "\n",
    "However, without context, this does not make sense too.\n",
    "\n",
    "#### Chunking\n",
    "The chunks here are defined as noun phrases, verb phrases, prepositional phrases and clauses. Of these, clauses are a combination of noun phrase and verb phrase. As we parse, we split the sentences into chunks defined by the rules above and select those that have proper nouns in them. This is a good approach but the caveat is that the if the tags are incorrectly labeled, then chunks can be misleading.\n",
    "\n",
    "Eg: \n",
    "legal team reviews each\n",
    "the ads delivered by Google\n",
    "we’re using information\n",
    "partners – like publishers\n",
    "security related materials\n",
    "Data generated through Google Analytics\n",
    "Google Analytics product helps businesses\n",
    "The hyperlinked examples\n",
    "don’t follow the correct process\n",
    "SMS routing information\n",
    "choice People have different privacy concerns\n",
    "\n",
    "Even in this case, while the results are better than frequency and collocation, the keyphrase in itself does not make much sense. \n",
    "\n",
    "#### Wordnet\n",
    "Using Wordnet to find the key topics in the Terms of Service, resulted in the following:\n",
    "['KEY TOPICS:', 'account', 'share', 'access', 'user', 'information', 'device']\n",
    "While these may be key topics, they give us no further information about the terms of service and the user is not better off having seen these topics. Wordnet does not perform very well in this case.\n",
    "\n",
    "#### Conclusion\n",
    "Our objective was that we need to summarize Terms of Service so that users can read the key points of the text and be better informed. None of the techniques used here to extract keyphrases facilitate this since the context of the phrase is missing and that is important to understand the sentence in its entirety.\n",
    "\n",
    "#### Learnings and Way Forward\n",
    "Rather than extract key phrases, our plan is to look at complete sentences so that the context is clear and the sentence is better comprehendable.\n",
    "\n",
    "In other techniques, we will focus on extracting complete sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Sentences Extraction Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Summarizing by picking sentences that contain most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code taken from Author: Tristan Havelick <tristan@havelick.com> URL: <https://github.com/thavelick/summarize/>\n",
    "\n",
    "class SimpleSummarizer:\n",
    "    def reorder_sentences(self, output_sentences, input ):\n",
    "        output_sentences.sort( lambda s1, s2:\n",
    "            input.find(s1) - input.find(s2) )\n",
    "        return output_sentences\n",
    "\n",
    "    def get_summarized(self, input, num_sentences ):\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "        # get the frequency of each word in the input\n",
    "        base_words = [word.lower() for word in tokenizer.tokenize(input)]\n",
    "        words = [word for word in base_words if word not in stopwords.words()]\n",
    "        word_frequencies = FreqDist(words)\n",
    "        \n",
    "        # now create a set of the most frequent words\n",
    "        most_frequent_words = [pair[0] for pair in\n",
    "            word_frequencies.items()[:100]]\n",
    "\n",
    "        # break the input up into sentences.  working_sentences is used\n",
    "        # for the analysis, but actual_sentences is used in the results\n",
    "        # so capitalization will be correct.\n",
    "        \n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        actual_sentences = sent_detector.tokenize(input)\n",
    "        working_sentences = [sentence.lower() for sentence in actual_sentences]\n",
    "\n",
    "        # iterate over the most frequent words, and add the first sentence\n",
    "        # that inclues each word to the result.\n",
    "        output_sentences = []\n",
    "\n",
    "        for word in most_frequent_words:\n",
    "            for i in range(0, len(working_sentences)):\n",
    "                if (word in working_sentences[i]\n",
    "                 and actual_sentences[i] not in output_sentences):\n",
    "                    output_sentences.append(actual_sentences[i])\n",
    "                    break\n",
    "                if len(output_sentences) >= num_sentences: break\n",
    "            if len(output_sentences) >= num_sentences: break\n",
    "                \n",
    "        # sort the output sentences back to their original order\n",
    "        return self.reorder_sentences(output_sentences, input)\n",
    "    \n",
    "    def summarize_text(self, input, num_sentences):\n",
    "        return self.get_summarized(input, num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Using our Services\n",
      "\n",
      "You must follow any policies made available to you within the Services.\n",
      "\n",
      "In connection with your use of the Services, we may send you service announcements, administrative messages, and other information.\n",
      "\n",
      "Do not use such Services in a way that distracts you and prevents you from obeying traffic or safety laws.\n",
      "\n",
      "The rights you grant in this license are for the limited purpose of operating, promoting, and improving our Services, and to develop new ones.\n",
      "\n",
      "Our automated systems analyze your content (including emails) to provide you personally relevant product features, such as customized search results, tailored advertising, and spam and malware detection.\n",
      "\n",
      "If you have a Google Account, we may display your Profile name, Profile photo, and actions you take on Google or on third-party applications connected to your Google Account (such as +1’s, reviews you write and comments you post) in our Services, including displaying in ads and other commercial contexts.\n",
      "\n",
      "You may not copy, modify, distribute, sell, or lease any part of our Services or included software, nor may you reverse engineer or attempt to extract the source code of that software, unless laws prohibit those restrictions or you have our written permission.\n",
      "\n",
      "You can stop using our Services at any time, although we’ll be sorry to see you go.\n",
      "\n",
      "It will hold harmless and indemnify Google and its affiliates, officers, agents, and employees from any claim, suit or action arising from or related to the use of the Services or violation of these terms, including any liability or expense arising from claims, losses, damages, suits, judgments, litigation costs and attorneys’ fees.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Summary:'\n",
    "\n",
    "ss=SimpleSummarizer()\n",
    "summary=ss.summarize_text(tos_text,10)\n",
    "for sent in summary[1:]:\n",
    "    print sent\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach pulled out nice sentences, but with Terms of Service, we cannot just rely on the most frequent terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Gensim summarization which works on TextRank Algorithm\n",
    "https://rare-technologies.com/text-summarization-with-gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "When you upload, submit, store, send or receive content to or through our Services, you give Google (and those we work with) a worldwide license to use, host, store, reproduce, modify, create derivative works (such as those resulting from translations, adaptations or other changes we make so that your content works better with our Services), communicate, publish, publicly perform, publicly display and distribute such content.\n",
      "\n",
      "You can find more information about how Google uses and stores content in the privacy policy or additional terms for particular Services.\n",
      "\n",
      "This license is for the sole purpose of enabling you to use and enjoy the benefit of the Services as provided by Google, in the manner permitted by these terms.\n",
      "\n",
      "TO THE EXTENT PERMITTED BY LAW, THE TOTAL LIABILITY OF GOOGLE, AND ITS SUPPLIERS AND DISTRIBUTORS, FOR ANY CLAIMS UNDER THESE TERMS, INCLUDING FOR ANY IMPLIED WARRANTIES, IS LIMITED TO THE AMOUNT YOU PAID US TO USE THE SERVICES (OR, IF WE CHOOSE, TO SUPPLYING YOU THE SERVICES AGAIN).\n",
      "\n",
      "It will hold harmless and indemnify Google and its affiliates, officers, agents, and employees from any claim, suit or action arising from or related to the use of the Services or violation of these terms, including any liability or expense arising from claims, losses, damages, suits, judgments, litigation costs and attorneys’ fees.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Summary:'\n",
    "tos_text = loadText(\"google.txt\")\n",
    "for sent in summarize(tos_text, split=True, ratio=.05):\n",
    "    print sent\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a nice and quite informative summary. We are getting the most important information that people would care about using this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: Classification and Summarization\n",
    "Classifying the text using words that are commonly used in the context of labels like copyright, privacy and termination and summarizing the text under each label using gensim summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Termination\n",
      "[u'If you are using a Google Account assigned to you by an administrator, different or additional terms may apply and your administrator may be able to access or disable your account.', u'We will respect the choices you make to limit sharing or visibility settings in your Google Account.']\n",
      "\n",
      "Copyright\n",
      "[u'Google gives you a personal, worldwide, royalty-free, non-assignable and non-exclusive license to use the software provided to you by Google as part of the Services.']\n",
      "\n",
      "Privacy\n",
      "[u'If you are using a Google Account assigned to you by an administrator, different or additional terms may apply and your administrator may be able to access or disable your account.', u'When you upload, submit, store, send or receive content to or through our Services, you give Google (and those we work with) a worldwide license to use, host, store, reproduce, modify, create derivative works (such as those resulting from translations, adaptations or other changes we make so that your content works better with our Services), communicate, publish, publicly perform, publicly display and distribute such content.', u'Also, in some of our Services, there are terms or settings that narrow the scope of our use of the content submitted in those Services.', u'You can find more information about how Google uses and stores content in the privacy policy or additional terms for particular Services.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def summarizeAlgo(_text): \n",
    "\n",
    "    tos_text_paras = _text.split(\"\\n\")\n",
    "    \n",
    "    # Classifying using common words used under labels \n",
    "    copyright = ['collective work',\\\n",
    "    'compilation',\\\n",
    "    'compulsory license',\\\n",
    "    'copyright',\\\n",
    "    'copyright holder/copyright owner',\\\n",
    "    'copyright notice',\\\n",
    "    'derivative work',\\\n",
    "    'exclusive right',\\\n",
    "    'expression',\\\n",
    "    'fair use',\\\n",
    "    'first sale doctrine',\\\n",
    "    'fixation',\\\n",
    "    'idea',\\\n",
    "    'infringement',\\\n",
    "    'intellectual property',\\\n",
    "    'license',\\\n",
    "    'master use license',\\\n",
    "    'mechanical license',\\\n",
    "    'medium',\\\n",
    "    'moral rights',\\\n",
    "    'musical composition',\\\n",
    "    'parody',\\\n",
    "    'patent',\\\n",
    "    'performing rights',\\\n",
    "    'permission',\\\n",
    "    'public domain',\\\n",
    "    'publication/publish',\\\n",
    "    'right of publicity',\\\n",
    "    'royalty',\\\n",
    "    'service mark',\\\n",
    "    'sound recording',\\\n",
    "    'statutory damages',\\\n",
    "    'synchronization license',\\\n",
    "    'tangible form of expression',\\\n",
    "    'term',\\\n",
    "    'title',\\\n",
    "    'trademark',\\\n",
    "    'trade secret',\\\n",
    "    'work for hire']\n",
    "\n",
    "    privacy = ['access',\\\n",
    "    'account',\\\n",
    "    'activity',\\\n",
    "    'advertising',\\\n",
    "    'confidentiality',\\\n",
    "    'content',\\\n",
    "    'cookie',\\\n",
    "    'legal',\\\n",
    "    'preferences',\\\n",
    "    'privacy',\\\n",
    "    'protect',\\\n",
    "    'religion',\\\n",
    "    'security',\\\n",
    "    'settings']\n",
    "\n",
    "    termination = ['cease',\\\n",
    "    'terminate',\\\n",
    "    'remove',\\\n",
    "    'inactive',\\\n",
    "    'suspend',\\\n",
    "    'account',\\\n",
    "    'discontinue',\\\n",
    "    'revoke',\\\n",
    "    'retain']\n",
    "\n",
    "    copyright_all,privacy_all,termination_all = [],[],[]\n",
    "\n",
    "\n",
    "    for para in tos_text_paras:\n",
    "        check = 0\n",
    "        for word in para.split(\" \"):\n",
    "            word = word.lower()\n",
    "            if word in copyright:\n",
    "                copyright_all.append(para)\n",
    "                check = 1\n",
    "            if word in privacy:\n",
    "                privacy_all.append(para)\n",
    "                check = 1\n",
    "            if word in termination:\n",
    "                termination_all.append(para)\n",
    "                check = 1\n",
    "            if check != 0:\n",
    "                break\n",
    "    \n",
    "    #Removing sentences that contain less than 5 words under all the labels\n",
    "    \n",
    "    copyright_all = [sent for sent in copyright_all if len(word_tokenize(sent)) > 5]\n",
    "\n",
    "    privacy_all = [sent for sent in privacy_all if len(word_tokenize(sent)) > 5]\n",
    "\n",
    "    termination_all = [sent for sent in termination_all if len(word_tokenize(sent)) > 5]\n",
    "\n",
    "    categoryDict = {}\n",
    "    \n",
    "    #Summarizing each labelled text\n",
    "\n",
    "    if (len(copyright_all) != 0):\n",
    "        if (len(copyright_all) != 1):\n",
    "            copyright_text = ' '.join(copyright_all)\n",
    "            copyright_all = summarize(copyright_text, split=True, ratio=.2)\n",
    "        categoryDict[\"Copyright\"] = copyright_all\n",
    "\n",
    "\n",
    "    if (len(privacy_all) != 0):\n",
    "        if (len(privacy_all) != 1):\n",
    "            privacy_text = ' '.join(privacy_all)\n",
    "            privacy_all = summarize(privacy_text, split=True, ratio=.1)\n",
    "        categoryDict[\"Privacy\"] = privacy_all\n",
    "\n",
    "\n",
    "    if (len(termination_all) != 0):\n",
    "        if (len(termination_all) != 1):\n",
    "            termination_text = ' '.join(termination_all)\n",
    "            termination_all = summarize(termination_text, split=True, ratio=.2)\n",
    "        categoryDict[\"Termination\"] = termination_all\n",
    "                              \n",
    "    return categoryDict\n",
    "\n",
    "\n",
    "summary = summarizeAlgo(tos_text)\n",
    "for key in summary:\n",
    "    print key\n",
    "    print summary[key]\n",
    "    print\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection on using Sentence Selection for ToS summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach #3 (Classification and Summarization) gave us the best results as we were able to divide the text into certain categories and within each category, we were able to pull out the most interesting and important sentences. As a user, we would want to know only the crux or the most important pieces of information in Terms of Services that would matter to us the most under Copyright, Privacy and Termination. We have picked this as our final algorithm. Hopefully, the surveys would help us in further evaluation of this approach.\n",
    "\n",
    "Going forward, we would like to rephrase the text to make the sentences shorter and easy to comprehend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
